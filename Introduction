# From the Abacus to the Transistor: The Path of Computation

## The Age of Mechanical Computation

To understand the need for computation and the path to modern electronic computing machines, one must start with the first calculating apparatus — the **Abacus**.

It allowed for storing a sum and easily performing addition and subtraction operations. The abacus consisted of a number of columns, each having ten countable elements. Each column represented a power of ten (units, tens, hundreds, thousands, ...). This design made it possible not to hold large numbers in one's head, as well as to save the state of calculations.

Since then, humanity has created a wide variety of computing devices: clocks of all kinds, the astrolabe, the slide rule, and many others.

The earliest mention of the word **"computer"** appeared in the 17th century. At that time, it referred to **people** who performed calculations manually.

---

## The First Machines

The first device to even remotely fit the description of a "computer" was the **Arithmometer**, invented by Thomas de Colmar (1820).  
It was a mechanical device with a handle that, when turned forward or backward, could perform addition and subtraction respectively.  
With a little ingenuity, it could also divide and multiply.

The mechanism consisted of geared rotating wheels, each with 10 teeth corresponding to the digits 0–9. After 10 rotations in one direction, the next wheel would shift by one tooth.  
It could perform all four basic arithmetic operations automatically, and its design was so successful that it was used for the next three centuries.

As the demand for calculations grew and exceeded the capabilities of such devices, **mathematical tables** were developed.  
These tables allowed one to find results for certain calculations without performing them manually — for example, square root tables or artillery firing tables during World War II.

---

## The Concept of the Difference Engine

The next major leap was the invention of the **Difference Engine**, conceived by **Charles Babbage** in 1822.  
Although he was far ahead of his time and could not build it with the technology available, in the early 20th century computer scientists managed to recreate it from his drawings — and it worked.

The machine weighed 15 tons and consisted of 25,000 parts. Its main advantage was the ability to work with **polynomials** — specifically, to approximate functions and model relationships between parameters.

While developing the Difference Engine, Babbage conceived the idea of the **Analytical Engine** — the first universal computer. It had **memory**, a **processor**, and even a **primitive printer**.  
By receiving data as input, it could perform sequential operations according to an instruction set.  
However, like its predecessor, it was never completed.

An English mathematician, **Ada Lovelace**, wrote the first **programs** for this theoretical Analytical Engine, effectively creating the first **programming language**.

---

## The Birth of Electromechanical Computing

A major impetus for practical automation came from the **U.S. census**.  
By law, it had to be conducted every 10 years. The first census took seven years to complete, and the next one was projected to take thirteen.  
This created a need for **automation**.

The U.S. government turned to **Herman Hollerith**, who designed an **electromechanical tabulating machine** using **punched cards**.  
Each card had holes representing data, and when inserted into the machine, electrical contacts would close corresponding circuits and turn mechanical counters.

This reduced the census time from a projected 13 years to just **2.5 years** and saved enormous sums of money.  
Hollerith’s company later became part of the **Tabulating Machine Company**, which eventually merged with others to form **IBM** — still one of the world’s leading players in computing technology.

This era demonstrated both the **necessity** and **demand** for automated computation. However, the existing electromechanical designs were bulky, expensive, and error-prone.

---

## From Relays to Tubes: The First Electronic Computers

One of the largest electromechanical computers of the time was **Harvard Mark I**, created by IBM in 1944 for the Allies during World War II.  
It contained thousands of components, hundreds of kilometers of wiring, and required a five-horsepower motor just to run its memory.

The “brain” of this machine was the **relay** — an electrically controlled mechanical switch.  
When current flowed through its coil, it created a magnetic field that attracted the switch, closing the circuit.  

However, relays had a fundamental flaw: **mechanical inertia**.  
They could switch only about **50 times per second**.  
As a result, Mark I could perform only:
- 3 additions or subtractions per second,  
- 1 multiplication every 6 seconds, and  
- 1 division every 15 seconds.

Relays also suffered from **wear and tear**, leading to constant maintenance.  
The term **"bug"** originated when a moth shorted out a relay in the Mark II.

---

## The Age of the Vacuum Tube

A major breakthrough came with **thermionic valves**, or **vacuum tubes**.

They were **thousands of times faster** than relays, since switching was now limited by the speed of electrons rather than mechanical movement.  
They were also more compact, allowing a dramatic leap in computing performance.

A vacuum tube operated as follows:
- A low-voltage circuit heated the **cathode**, releasing electrons.  
- A high-voltage circuit connected to the **anode** attracted these electrons.  
- A **control grid** between them could either block or allow the flow by adjusting its voltage.

If the grid was negatively charged, it repelled electrons and **blocked** the circuit.  
If it was neutral or slightly positive, current flowed, and the tube was **on**.

Despite their advantages, tubes consumed a lot of power, constantly required heating, and tended to **overheat** and **burn out**.  
Still, their **speed and compactness** made them the foundation of early electronic computers, radios, and telephones.

By the mid-20th century, vacuum tubes had become more reliable and affordable, allowing governments and corporations to build large-scale computers.  
This marked the transition from **electromechanical** to **electronic computing**.

---

## The Invention of the Transistor

Even vacuum tubes eventually reached their limits.  
The need for a more compact, efficient, and reliable solution led to the invention of the **transistor** in **1947** at Bell Labs.

The principle was similar: control the flow of current — but now through **semiconductors**.  
A small voltage applied to the **gate** (or base) changed the conductivity between two other terminals, allowing or blocking current flow.

The first prototypes could switch **10,000 times per second**, an incredible achievement at the time.  
More importantly, transistors were **solid-state components** — no fragile filaments, no mechanical parts, and far greater reliability.

Very quickly, transistors became **smaller and cheaper**, decisively replacing vacuum tubes.  
This made computers a **mass-market product**, starting with the **IBM 608**, the first all-transistorized calculator.

Today’s transistors are **thousands of times smaller than the thickness of a sheet of paper** and can switch millions of times per second, with a lifespan measured in decades.

---

## The End of Moore’s Law

For decades, the number of transistors on a chip doubled roughly every two years — a pattern known as **Moore’s Law**.  
However, this exponential growth has slowed dramatically as we approach the physical limits of **miniaturization**.

This has spurred research into **new computing paradigms**, the most promising of which is **quantum computing** — a field that may one day revolutionize the concept of computation itself.
